{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal convolucional avanzada para detectar objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Número de imágenes\n",
    "output_every = 50\n",
    "generations = 20000 # Número de iteraciones\n",
    "eval_every = 500\n",
    "\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "\n",
    "crop_height = 24 # Tamaño al que cambiaremos la images para que la red neuronal aprenda todavía más\n",
    "crop_width = 24\n",
    "\n",
    "num_channels = 3 # Será imágenes rgb\n",
    "num_targets = 10\n",
    "\n",
    "data_folder = \"cifar-10-batches-bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Learning\\ rate = 0.1 \\cdot 0.9^{\\frac{x}{250}} $$\n",
    "\n",
    "Empieza en 0.1 y baja un 10% cada 250 iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "lr_decay = 0.9\n",
    "num_generations_to_wait = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vect_length = image_width*image_height*num_channels\n",
    "record_lenght = 1 + image_vect_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descarga y procesamiento de CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio donde queremos guardar las imágenes\n",
    "data_dir = \"../../datasets/cifar-10-temp\"\n",
    "\n",
    "# Si no existe el directorio, lo crea\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Url desde la que descargamos los datos\n",
    "cifar_10_url = \"http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\"\n",
    "# Nombre del fichero comprimido descargado\n",
    "data_file = os.path.join(data_dir, \"cifar-10-binary.tar.gz\")\n",
    "\n",
    "# Si el fichero no existe\n",
    "if not os.path.isfile(data_file):\n",
    "    # Guarda el fichero de la url 'cifar-10-url' en la ruta 'data_file'\n",
    "    file_path, _ = urllib.request.urlretrieve(url=cifar_10_url, filename=data_file)\n",
    "    # Extraer los ficheros del archivo comprimido\n",
    "    tarfile.open(file_path, \"r:gz\").extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar_files(filename_queue, distord_images=True):\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_lenght)\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    # Creamos fichero binario\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    # Extraemos la etiqueta\n",
    "    image_label = tf.cast(tf.slice(record_bytes, begin=[0], size=[1]), tf.int32)\n",
    "    # Extraemos la imagen\n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes, begin=[1], size=[image_vect_length]), \n",
    "                                 [num_channels, image_height, image_width])\n",
    "    # Redimensión de imagen\n",
    "    reshaped_image = tf.transpose(image_extracted, [1,2,0])\n",
    "    reshaped_image = tf.cast(reshaped_image, tf.float32)\n",
    "    \n",
    "    # Crop (corte) aleatorio\n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(image=reshaped_image, \n",
    "                                                         target_height=crop_height, \n",
    "                                                         target_width=crop_width)\n",
    "    \n",
    "    if distord_images:\n",
    "        # Flip horizontal aleatorio, cambios brillo y contraste\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        final_image = tf.image.random_brightness(final_image, max_delta=63)\n",
    "        final_image = tf.image.random_contrast(final_image, lower=0.2, upper=1.8)\n",
    "    \n",
    "    # Estandarización por color\n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "    \n",
    "    return final_image, image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener los conjuntos de imágenes\n",
    "def input_pipeline(batch_size, train_logical=True):\n",
    "    # Si estamos en entrenamiento\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, data_folder, \"data_batch_{}.bin\".format(i)) for i in range(1,6)]\n",
    "    # Si estamos en testing\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, data_folder, \"test_batch.bin\")]\n",
    "        \n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image, label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    # Tamaño mínimo del buffer para poder cargar y muestrear la imagen\n",
    "    min_after_dequeue = 1000\n",
    "    capacity = min_after_dequeue + 3*batch_size # número de hilos + margen*batch_size\n",
    "    \n",
    "    example_batch, label_batch = tf.train.shuffle_batch(tensors=[image, label], \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        capacity=capacity, \n",
    "                                                        min_after_dequeue=min_after_dequeue)\n",
    "    \n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 Redes de convolución \n",
    "    - 64 nodos cada una\n",
    "- 3 capas totalmente conectadas\n",
    "    - 384 nodos la primera\n",
    "    - 192 nodos la segunda\n",
    "    - 10 clases en la capa final (predicción)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, \n",
    "                               shape=shape, \n",
    "                               dtype=dtype, \n",
    "                               initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    def zero_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    # Primera capa de convolución\n",
    "    with tf.variable_scope(\"conv1\") as scope:\n",
    "        # filtro de convolución de 5x5 para 3 canales de color de entrada y 64 nodos de salida\n",
    "        conv1_kernel = truncated_normal_var(name=\"conv_kernel1\", shape=[5,5,num_channels,64], dtype=tf.float32)\n",
    "        conv1 = tf.nn.conv2d(input=input_images, filter=conv1_kernel, strides=[1,1,1,1], padding=\"SAME\")\n",
    "        conv1_bias = zero_var(name=\"conv_bias1\", shape=[64], dtype=tf.float32)\n",
    "        conv1 = tf.nn.bias_add(value=conv1, bias=conv1_bias)\n",
    "        # Capa ReLu\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        \n",
    "    # Max pooling de 3x3 y desplazamiento 2x2\n",
    "    pool1 = tf.nn.max_pool(value=relu1, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\", name=\"pool_layer1\")\n",
    "    \n",
    "    # Normalización de las imágenes\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm1\")\n",
    "    \n",
    "    # Segunda capa de convolución\n",
    "    with tf.variable_scope(\"conv2\") as scope:\n",
    "        # filtro de convolución de 5x5 para 64 nodos de entrada y 64 nodos de salida\n",
    "        conv2_kernel = truncated_normal_var(name=\"conv_kernel2\", shape=[5,5,64,64], dtype=tf.float32)\n",
    "        conv2 = tf.nn.conv2d(input=norm1, filter=conv2_kernel, strides=[1,1,1,1], padding=\"SAME\")\n",
    "        conv2_bias = zero_var(name=\"conv_bias2\", shape=[64], dtype=tf.float32)\n",
    "        conv2 = tf.nn.bias_add(value=conv2, bias=conv2_bias)\n",
    "        # Capa ReLu\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        \n",
    "    # Max pooling de 3x3 y desplazamiento 2x2\n",
    "    pool2 = tf.nn.max_pool(value=relu2, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\", name=\"pool_layer2\")\n",
    "    \n",
    "    # Normalización de las imágenes\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm2\")\n",
    "    \n",
    "    # Redimensionar a una matriz para poder multiplicar en las capas totalmente conectadas\n",
    "    reshaped_output = tf.reshape(norm2, [batch_size, -1]) # Vector columna\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "    \n",
    "    # Primera capa totalmente conectada\n",
    "    with tf.variable_scope(\"full1\") as scope:\n",
    "        weight_full_layer1 = truncated_normal_var(name=\"weight_full_layer1\", \n",
    "                                                  shape=[reshaped_dim, 384], \n",
    "                                                  dtype=tf.float32)\n",
    "        bias_full_layer1 = zero_var(name=\"bias_full_layer1\", shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, weight_full_layer1), bias_full_layer1))\n",
    "    \n",
    "    # Segunda capa totalmente conectada\n",
    "    with tf.variable_scope(\"full2\") as scope:\n",
    "        weight_full_layer2 = truncated_normal_var(name=\"weight_full_layer2\", \n",
    "                                                  shape=[384, 192], \n",
    "                                                  dtype=tf.float32)\n",
    "        bias_full_layer2 = zero_var(name=\"bias_full_layer2\", shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, weight_full_layer2), bias_full_layer2))\n",
    "        \n",
    "    # Última capa totalmente conectada\n",
    "    with tf.variable_scope(\"full3\") as scope:\n",
    "        weight_full_layer3 = truncated_normal_var(name=\"weight_full_layer3\", \n",
    "                                                  shape=[192, num_targets], \n",
    "                                                  dtype=tf.float32)\n",
    "        bias_full_layer3 = zero_var(name=\"bias_full_layer3\", shape=[num_targets], dtype=tf.float32)\n",
    "        final_output = tf.add(tf.matmul(full_layer2, weight_full_layer3), bias_full_layer3)\n",
    "        \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loss(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    return tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss_value, generation_number):\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate=learning_rate, \n",
    "                                                     global_step=generation_number,\n",
    "                                                     decay_rate=lr_decay, \n",
    "                                                     decay_steps=num_generations_to_wait, \n",
    "                                                     staircase=True)\n",
    "    \n",
    "    return tf.train.GradientDescentOptimizer(model_learning_rate).minimize(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    predictions = tf.cast(tf.argmax(logits, axis=1), tf.int32)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, targets), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-5392311cac20>:8: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-a590bed1f3ac>:2: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From <ipython-input-8-5392311cac20>:18: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "# No necesitamos placeholders, ya que queda todo perfectamente definido\n",
    "images, targets = input_pipeline(batch_size, train_logical=True)\n",
    "test_images, test_targets = input_pipeline(batch_size, train_logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"model_definition\") as scope:\n",
    "    model_output = cifar_cnn_model(images, batch_size)\n",
    "    scope.reuse_variables()\n",
    "    test_output = cifar_cnn_model(test_images, batch_size, train_logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cifar_loss(model_output, targets)\n",
    "accuracy = get_accuracy(test_output, test_targets)\n",
    "generation_num = tf.Variable(initial_value=0, trainable=False) # El grafo no modificará este valor\n",
    "train_operation = train(loss, generation_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-978b974a4591>:2: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 123145579294720)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145584549888)>,\n",
       " <Thread(QueueRunnerThread-input_producer_1-input_producer_1/input_producer_1_EnqueueMany, started daemon 123145589805056)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_1/random_shuffle_queue-shuffle_batch_1/random_shuffle_queue_enqueue, started daemon 123145595060224)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iniciamos los hilos de ejecución en paralelo\n",
    "tf.train.start_queue_runners(sess=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso #50 ==> Loss: 1.92823\n",
      "Paso #100 ==> Loss: 1.95564\n",
      "Paso #150 ==> Loss: 1.73939\n",
      "Paso #200 ==> Loss: 1.54637\n",
      "Paso #250 ==> Loss: 1.49652\n",
      "Paso #300 ==> Loss: 1.68269\n",
      "Paso #350 ==> Loss: 1.35595\n",
      "Paso #400 ==> Loss: 1.53400\n",
      "Paso #450 ==> Loss: 1.41719\n",
      "Paso #500 ==> Loss: 1.32514\n",
      "--- Precisión en test 0.52 ---\n",
      "Paso #550 ==> Loss: 1.39472\n",
      "Paso #600 ==> Loss: 1.27843\n",
      "Paso #650 ==> Loss: 1.51472\n",
      "Paso #700 ==> Loss: 1.39203\n",
      "Paso #750 ==> Loss: 1.40739\n",
      "Paso #800 ==> Loss: 1.16718\n",
      "Paso #850 ==> Loss: 1.20609\n",
      "Paso #900 ==> Loss: 1.14297\n",
      "Paso #950 ==> Loss: 1.27602\n",
      "Paso #1000 ==> Loss: 1.20972\n",
      "--- Precisión en test 0.54 ---\n",
      "Paso #1050 ==> Loss: 1.13170\n",
      "Paso #1100 ==> Loss: 1.35724\n",
      "Paso #1150 ==> Loss: 1.11064\n",
      "Paso #1200 ==> Loss: 1.03331\n",
      "Paso #1250 ==> Loss: 1.17287\n",
      "Paso #1300 ==> Loss: 1.09078\n",
      "Paso #1350 ==> Loss: 0.95662\n",
      "Paso #1400 ==> Loss: 1.05994\n",
      "Paso #1450 ==> Loss: 1.10032\n",
      "Paso #1500 ==> Loss: 0.97102\n",
      "--- Precisión en test 0.54 ---\n",
      "Paso #1550 ==> Loss: 0.88629\n",
      "Paso #1600 ==> Loss: 0.93358\n",
      "Paso #1650 ==> Loss: 1.05742\n",
      "Paso #1700 ==> Loss: 0.95327\n",
      "Paso #1750 ==> Loss: 0.87838\n",
      "Paso #1800 ==> Loss: 0.99200\n",
      "Paso #1850 ==> Loss: 0.99793\n",
      "Paso #1900 ==> Loss: 0.95708\n",
      "Paso #1950 ==> Loss: 0.80981\n",
      "Paso #2000 ==> Loss: 0.98382\n",
      "--- Precisión en test 0.66 ---\n",
      "Paso #2050 ==> Loss: 1.09426\n",
      "Paso #2100 ==> Loss: 0.79069\n",
      "Paso #2150 ==> Loss: 0.80495\n",
      "Paso #2200 ==> Loss: 0.73333\n",
      "Paso #2250 ==> Loss: 0.81540\n",
      "Paso #2300 ==> Loss: 1.02973\n",
      "Paso #2350 ==> Loss: 0.91137\n",
      "Paso #2400 ==> Loss: 0.76241\n",
      "Paso #2450 ==> Loss: 0.85955\n",
      "Paso #2500 ==> Loss: 0.74645\n",
      "--- Precisión en test 0.69 ---\n",
      "Paso #2550 ==> Loss: 0.77229\n",
      "Paso #2600 ==> Loss: 0.58394\n",
      "Paso #2650 ==> Loss: 0.77654\n",
      "Paso #2700 ==> Loss: 0.83722\n",
      "Paso #2750 ==> Loss: 0.81743\n",
      "Paso #2800 ==> Loss: 0.60253\n",
      "Paso #2850 ==> Loss: 0.95896\n",
      "Paso #2900 ==> Loss: 0.73632\n",
      "Paso #2950 ==> Loss: 0.81701\n",
      "Paso #3000 ==> Loss: 0.64221\n",
      "--- Precisión en test 0.70 ---\n",
      "Paso #3050 ==> Loss: 0.71572\n",
      "Paso #3100 ==> Loss: 0.61438\n",
      "Paso #3150 ==> Loss: 0.58808\n",
      "Paso #3200 ==> Loss: 0.49905\n",
      "Paso #3250 ==> Loss: 0.59524\n",
      "Paso #3300 ==> Loss: 0.75515\n",
      "Paso #3350 ==> Loss: 0.79967\n",
      "Paso #3400 ==> Loss: 0.72906\n",
      "Paso #3450 ==> Loss: 0.73573\n",
      "Paso #3500 ==> Loss: 0.77243\n",
      "--- Precisión en test 0.58 ---\n",
      "Paso #3550 ==> Loss: 0.54937\n",
      "Paso #3600 ==> Loss: 0.57773\n",
      "Paso #3650 ==> Loss: 0.69628\n",
      "Paso #3700 ==> Loss: 0.53629\n",
      "Paso #3750 ==> Loss: 0.76154\n",
      "Paso #3800 ==> Loss: 0.64085\n",
      "Paso #3850 ==> Loss: 0.67592\n",
      "Paso #3900 ==> Loss: 0.77565\n",
      "Paso #3950 ==> Loss: 0.55103\n",
      "Paso #4000 ==> Loss: 0.49020\n",
      "--- Precisión en test 0.77 ---\n",
      "Paso #4050 ==> Loss: 0.64570\n",
      "Paso #4100 ==> Loss: 0.76900\n",
      "Paso #4150 ==> Loss: 0.49042\n",
      "Paso #4200 ==> Loss: 0.62382\n",
      "Paso #4250 ==> Loss: 0.61139\n",
      "Paso #4300 ==> Loss: 0.53935\n",
      "Paso #4350 ==> Loss: 0.59740\n",
      "Paso #4400 ==> Loss: 0.57414\n",
      "Paso #4450 ==> Loss: 0.52039\n",
      "Paso #4500 ==> Loss: 0.47762\n",
      "--- Precisión en test 0.70 ---\n",
      "Paso #4550 ==> Loss: 0.65207\n",
      "Paso #4600 ==> Loss: 0.60837\n",
      "Paso #4650 ==> Loss: 0.70006\n",
      "Paso #4700 ==> Loss: 0.46521\n",
      "Paso #4750 ==> Loss: 0.41917\n",
      "Paso #4800 ==> Loss: 0.54365\n",
      "Paso #4850 ==> Loss: 0.58258\n",
      "Paso #4900 ==> Loss: 0.74822\n",
      "Paso #4950 ==> Loss: 0.71771\n",
      "Paso #5000 ==> Loss: 0.77629\n",
      "--- Precisión en test 0.66 ---\n",
      "Paso #5050 ==> Loss: 0.57133\n",
      "Paso #5100 ==> Loss: 0.49568\n",
      "Paso #5150 ==> Loss: 0.56041\n",
      "Paso #5200 ==> Loss: 0.51058\n",
      "Paso #5250 ==> Loss: 0.31874\n",
      "Paso #5300 ==> Loss: 0.48015\n",
      "Paso #5350 ==> Loss: 0.52842\n",
      "Paso #5400 ==> Loss: 0.65963\n",
      "Paso #5450 ==> Loss: 0.56494\n",
      "Paso #5500 ==> Loss: 0.50548\n",
      "--- Precisión en test 0.70 ---\n",
      "Paso #5550 ==> Loss: 0.38843\n",
      "Paso #5600 ==> Loss: 0.57231\n",
      "Paso #5650 ==> Loss: 0.45750\n",
      "Paso #5700 ==> Loss: 0.41860\n",
      "Paso #5750 ==> Loss: 0.56702\n",
      "Paso #5800 ==> Loss: 0.46928\n",
      "Paso #5850 ==> Loss: 0.40254\n",
      "Paso #5900 ==> Loss: 0.45634\n",
      "Paso #5950 ==> Loss: 0.37548\n",
      "Paso #6000 ==> Loss: 0.45263\n",
      "--- Precisión en test 0.76 ---\n",
      "Paso #6050 ==> Loss: 0.64503\n",
      "Paso #6100 ==> Loss: 0.62564\n",
      "Paso #6150 ==> Loss: 0.41457\n",
      "Paso #6200 ==> Loss: 0.49157\n",
      "Paso #6250 ==> Loss: 0.44706\n",
      "Paso #6300 ==> Loss: 0.34108\n",
      "Paso #6350 ==> Loss: 0.44171\n",
      "Paso #6400 ==> Loss: 0.40221\n",
      "Paso #6450 ==> Loss: 0.46010\n",
      "Paso #6500 ==> Loss: 0.34230\n",
      "--- Precisión en test 0.73 ---\n",
      "Paso #6550 ==> Loss: 0.47014\n",
      "Paso #6600 ==> Loss: 0.41296\n",
      "Paso #6650 ==> Loss: 0.38091\n",
      "Paso #6700 ==> Loss: 0.41301\n",
      "Paso #6750 ==> Loss: 0.49305\n",
      "Paso #6800 ==> Loss: 0.27674\n",
      "Paso #6850 ==> Loss: 0.34230\n",
      "Paso #6900 ==> Loss: 0.45354\n",
      "Paso #6950 ==> Loss: 0.40914\n",
      "Paso #7000 ==> Loss: 0.38352\n",
      "--- Precisión en test 0.70 ---\n",
      "Paso #7050 ==> Loss: 0.39988\n",
      "Paso #7100 ==> Loss: 0.37940\n",
      "Paso #7150 ==> Loss: 0.38312\n",
      "Paso #7200 ==> Loss: 0.39311\n",
      "Paso #7250 ==> Loss: 0.45541\n",
      "Paso #7300 ==> Loss: 0.43038\n",
      "Paso #7350 ==> Loss: 0.33890\n",
      "Paso #7400 ==> Loss: 0.40292\n",
      "Paso #7450 ==> Loss: 0.34073\n",
      "Paso #7500 ==> Loss: 0.32910\n",
      "--- Precisión en test 0.77 ---\n",
      "Paso #7550 ==> Loss: 0.33434\n",
      "Paso #7600 ==> Loss: 0.32042\n",
      "Paso #7650 ==> Loss: 0.25531\n",
      "Paso #7700 ==> Loss: 0.36019\n",
      "Paso #7750 ==> Loss: 0.40149\n",
      "Paso #7800 ==> Loss: 0.44983\n",
      "Paso #7850 ==> Loss: 0.14841\n",
      "Paso #7900 ==> Loss: 0.28840\n",
      "Paso #7950 ==> Loss: 0.36425\n",
      "Paso #8000 ==> Loss: 0.23913\n",
      "--- Precisión en test 0.72 ---\n",
      "Paso #8050 ==> Loss: 0.37138\n",
      "Paso #8100 ==> Loss: 0.33915\n",
      "Paso #8150 ==> Loss: 0.36420\n",
      "Paso #8200 ==> Loss: 0.37033\n",
      "Paso #8250 ==> Loss: 0.38695\n",
      "Paso #8300 ==> Loss: 0.26074\n",
      "Paso #8350 ==> Loss: 0.37531\n",
      "Paso #8400 ==> Loss: 0.29847\n",
      "Paso #8450 ==> Loss: 0.30409\n",
      "Paso #8500 ==> Loss: 0.37070\n",
      "--- Precisión en test 0.77 ---\n",
      "Paso #8550 ==> Loss: 0.45306\n",
      "Paso #8600 ==> Loss: 0.42647\n",
      "Paso #8650 ==> Loss: 0.23334\n",
      "Paso #8700 ==> Loss: 0.30164\n",
      "Paso #8750 ==> Loss: 0.21870\n",
      "Paso #8800 ==> Loss: 0.22777\n",
      "Paso #8850 ==> Loss: 0.25339\n",
      "Paso #8900 ==> Loss: 0.23345\n",
      "Paso #8950 ==> Loss: 0.31043\n",
      "Paso #9000 ==> Loss: 0.29147\n",
      "--- Precisión en test 0.69 ---\n",
      "Paso #9050 ==> Loss: 0.27226\n",
      "Paso #9100 ==> Loss: 0.44563\n",
      "Paso #9150 ==> Loss: 0.38093\n"
     ]
    }
   ],
   "source": [
    "loss_vector = []\n",
    "test_acc = []\n",
    "\n",
    "for i in range(generations):\n",
    "    _, loss_value = session.run([train_operation, loss])\n",
    "    \n",
    "    if (i+1)%output_every == 0:\n",
    "        loss_vector.append(loss_value)\n",
    "        print(\"Paso #{} ==> Loss: {:.5f}\".format(i+1, loss_value))\n",
    "\n",
    "    if (i+1)%eval_every == 0:\n",
    "        [temp_acc] = session.run([accuracy])\n",
    "        test_acc.append(temp_acc)\n",
    "        print(\"--- Precisión en test {:.2f} ---\".format(temp_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_idx = range(0, generations, eval_every)\n",
    "output_idx = range(0, generations, output_every)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(output_idx, loss_vector)\n",
    "plt.title(\"Softmax Loss\", fontsize=15)\n",
    "plt.xlabel(\"Iteración\", fontsize=14)\n",
    "plt.ylabel(\"Pérdidas\", fontsize=14)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(eval_idx, test_acc)\n",
    "plt.title(\"Función de precisión\", fontsize=15)\n",
    "plt.xlabel(\"Iteración\", fontsize=14)\n",
    "plt.ylabel(\"Precisión\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
